{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "colab": {
   "name": "tut05.ipynb",
   "provenance": [],
   "include_colab_link": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/davideandres95/ml_comm/blob/main/tut05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5400c85e"
   },
   "source": [
    "#  Tutorial 5: Equalization in Random Noise\n",
    "November 18, 2021"
   ],
   "id": "5400c85e"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0aeb25d5"
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim"
   ],
   "id": "0aeb25d5",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "610e92ce"
   },
   "source": [
    "### Problem 5.2 - NN Equalizers for attenuated BPSK in AWGN\n",
    "In this problem, you\n",
    "train various equalizers for channel (5.4) and you compare the theoretical findings of\n",
    "Section 5.1 to the input-output characteristics of trained NN equalizers."
   ],
   "id": "610e92ce"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1d3a57a"
   },
   "source": [
    "##### 1. Linear MSE equalizer: \n",
    "implement a linear NN equalizer $f$ consisting of a single\n",
    "linear neuron w.r.t. to the and train it w.r.t. the MSE loss $|X-f(\\tilde{Y})|$. \n",
    "Plot the\n",
    "I/O function $f$ of the trained NN and compare it to (5.21)."
   ],
   "id": "f1d3a57a"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "edc5e3e4"
   },
   "source": [
    "def mapper(bits, alphabet):\n",
    "    return alphabet[bits]\n",
    "\n",
    "def awgn_channel(x, snr, seed=None):\n",
    "    rng = np.random.Generator(np.random.PCG64(seed))\n",
    "    power_x = np.mean(np.abs(x) ** 2)\n",
    "    noise_power = power_x / snr\n",
    "    noise = np.sqrt(noise_power) * rng.normal(size=x.shape) \n",
    "    return x + noise"
   ],
   "id": "edc5e3e4",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dae8d31e"
   },
   "source": [
    "# Channel model\n",
    "M = 2 # cardinality of the alphabet\n",
    "b = np.random.choice(M, 10000) # bits\n",
    "x = mapper(b, np.array([ -1., 1.])) # symbols\n",
    "gamma = 0.7\n",
    "SNRdB = 0\n",
    "snr = 10**(SNRdB/10)"
   ],
   "id": "dae8d31e",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bafd970c"
   },
   "source": [
    "class LinearEq(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.out = nn.Linear(1,1)\n",
    "\n",
    "    def forward(self, y):\n",
    "        y = self.out(y)\n",
    "        return y"
   ],
   "id": "bafd970c",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "74915bb2"
   },
   "source": [
    "# Initialize Network\n",
    "eq = LinearEq()\n",
    "# Define loss function and optimizer\n",
    "loss_fn_mse = nn.MSELoss()\n",
    "optimizer = optim.Adam(eq.parameters(), lr=1e-2)"
   ],
   "id": "74915bb2",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ee703bcc"
   },
   "source": [
    "y = gamma* awgn_channel(x,snr)\n",
    "y_t = torch.tensor(y.reshape(-1,1)).float()\n",
    "x_t = torch.tensor(x.reshape(1,-1)).float()\n",
    "# Trainings loop\n",
    "for j in range(1000):\n",
    "    x_hat = eq(y_t).reshape(1, -1)\n",
    "    loss = loss_fn_mse(x_hat, x_t)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Printout and visualization\n",
    "    if j % 50 == 0:\n",
    "        print(f'epoch {j}: Loss = {loss.detach().numpy() :.4f}')\n",
    "\n"
   ],
   "id": "ee703bcc",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7fb56919"
   },
   "source": [
    "# Plot\n",
    "yy = np.arange(-2.1,2.1,0.1)\n",
    "f_y = eq(torch.tensor(yy.reshape(-1,1)).float()).detach().numpy().reshape(-1,)\n",
    "plt.plot(yy, f_y, label = 'NN output')\n",
    "plt.plot(yy, 1/gamma/(1+1/snr)*yy, ':', label='expected')\n",
    "plt.plot(yy, yy/gamma, ':', label='optimal')\n",
    "plt.legend()\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('f(y)')\n",
    "plt.grid()"
   ],
   "id": "7fb56919",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffe43e38"
   },
   "source": [
    "##### 2. Non-linear MSE equalizer\n",
    "Implement a non-linear NN equalizer f consisting of several hidden layers with non-linear activations and train it w.r.t. the MSE loss $|X-f(\\tilde{Y})|$. Plot the I/O function $f$ of the trained NN and compare it to (5.24)."
   ],
   "id": "ffe43e38"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2edb263d"
   },
   "source": [
    "class NonLinearEq(nn.Module):\n",
    "    def __init__(self, n): \n",
    "        super().__init__()\n",
    "        self.h1 = nn.Linear(1,n)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.h2 = nn.Linear(n,n)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.h3 = nn.Linear(n,n)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.out = nn.Linear(n,1)\n",
    "\n",
    "    def forward(self, y):\n",
    "        y = self.act1(self.h1(y))\n",
    "        y = self.act2(self.h2(y))\n",
    "        y = self.act3(self.h3(y))\n",
    "        return self.out(y)"
   ],
   "id": "2edb263d",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "f7b53329"
   },
   "source": [
    "# Initialize Network\n",
    "eq_nl = NonLinearEq(10)\n",
    "# Define loss function and optimizer\n",
    "loss_fn_mse = nn.MSELoss()\n",
    "optimizer = optim.Adam(eq_nl.parameters(), lr=1e-2)"
   ],
   "id": "f7b53329",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "15bf833e"
   },
   "source": [
    "y = gamma* awgn_channel(x,snr)\n",
    "y_t = torch.tensor(y.reshape(-1,1)).float()\n",
    "x_t = torch.tensor(x.reshape(1,-1)).float()\n",
    "# Trainings loop\n",
    "for j in range(1000):\n",
    "    x_hat = eq_nl(y_t).reshape(1, -1)\n",
    "    loss = loss_fn_mse(x_hat, x_t)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Printout and visualization\n",
    "    if j % 50 == 0:\n",
    "        print(f'epoch {j}: Loss = {loss.detach().numpy() :.4f}')\n"
   ],
   "id": "15bf833e",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6740c7f7"
   },
   "source": [
    "# Plot\n",
    "f_y = eq_nl(torch.tensor(yy.reshape(-1,1)).float()).detach().numpy().reshape(-1,)\n",
    "plt.plot(yy, f_y, label = 'NN output')\n",
    "plt.plot(yy, np.tanh(yy*snr/gamma),':', label='expected')\n",
    "plt.plot(yy, yy/gamma, ':', label='optimal')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('f(y)')\n",
    "plt.legend()\n",
    "plt.grid()"
   ],
   "id": "6740c7f7",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8e729c3d"
   },
   "source": [
    "##### 3. Optimal equalizer \n",
    "Use the same non-linear NN as in 2. as equalizer. Concatenate\n",
    "it with a single linear neuron demapper whose parameters you fix according\n",
    "to Section 5.1.1. Train the NN equalizer across the demapper proxy w.r.t. the\n",
    "BCE $\\text{bce}_{\\log}(B,L)$. Plot the I/O function f of the trained NN and compare it to\n",
    "(5.9)."
   ],
   "id": "8e729c3d"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b6e74817"
   },
   "source": [
    "# Initialize Network\n",
    "eq_nl = NonLinearEq(10)\n",
    "# Define loss function and optimizer\n",
    "loss_fn_bce = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(eq_nl.parameters(), lr=0.001)"
   ],
   "id": "b6e74817",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1401df20"
   },
   "source": [
    "y = gamma* awgn_channel(x,snr)\n",
    "sigma2 = 1/snr\n",
    "y_t = torch.Tensor(y.reshape(-1,1))\n",
    "b_t = torch.Tensor(b.reshape((-1,1)))\n",
    "# Trainings loop\n",
    "for j in range(1000):\n",
    "    x_hat = eq_nl(y_t)\n",
    "    loss = loss_fn_bce(2/sigma2 * x_hat, b_t)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Printout and visualization\n",
    "    if j % 50 == 0:\n",
    "        print(f'epoch {j}: Loss = {loss.detach().numpy() :.4f}')"
   ],
   "id": "1401df20",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "65ace33d"
   },
   "source": [
    "# Plot\n",
    "f_y = eq_nl(torch.tensor(yy.reshape(-1,1)).float()).detach().numpy().reshape(-1,)\n",
    "\n",
    "plt.plot(yy, f_y, label = 'NN output')\n",
    "plt.plot(yy, yy/gamma,':', label='optimal')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('f(y)')\n",
    "plt.legend()\n",
    "plt.grid()"
   ],
   "id": "65ace33d",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab95e007"
   },
   "source": [
    "#### 4.\n",
    "Use the NN equalizer from 1.-3. concatenated with the single linear neuron demapper\n",
    "fixed according to Section 5.1.1 to verify the curves in Figure 5.4."
   ],
   "id": "ab95e007"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9dc6001d"
   },
   "source": [
    "SNRdBs = np.arange(0,10)\n",
    "SNRs = 10**(SNRdBs/10)\n",
    "eq_lin = []\n",
    "eq_nonlin = []\n",
    "eq_opt = []\n",
    "for snr in SNRs:\n",
    "    print(f'---- SNR is: {snr:.2f}')\n",
    "    y = gamma* awgn_channel(x,snr)\n",
    "    sigma2 = 1/snr\n",
    "    y_t = torch.tensor(y.reshape(-1,1)).float()\n",
    "    eq = LinearEq()\n",
    "    optimizer = optim.Adam(eq.parameters(), lr=0.01)\n",
    "    \n",
    "    # Linear MSE\n",
    "    # Trainings loop\n",
    "    for j in range(1000):\n",
    "        f_y_lin = eq(y_t).reshape(1, -1)\n",
    "        loss = loss_fn_mse(f_y_lin, x_t)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Printout and visualization\n",
    "        #if j % 50 == 0:\n",
    "            #print(f'epoch {j}: Loss = {loss.detach().numpy() :.4f}')\n",
    "    \n",
    "    l = 2/sigma2 * f_y_lin.reshape(-1, 1)\n",
    "    equivocation = loss_fn_bce(l, b_t).detach().numpy()/np.log(2)\n",
    "    eq_lin.append(equivocation)\n",
    "    print(f'equivocation linear {equivocation:.2f}')\n",
    "\n",
    "    # Nonlinear MSE\n",
    "    eq_nl = NonLinearEq(10)\n",
    "    optimizer = optim.Adam(eq_nl.parameters(), lr=0.01)\n",
    "    # Trainings loop\n",
    "    for j in range(1000):\n",
    "        f_y_nl = eq_nl(y_t).reshape(1, -1)\n",
    "        loss = loss_fn_mse(f_y_nl, x_t)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Printout and visualization\n",
    "        #if j % 50 == 0:\n",
    "            #print(f'epoch {j}: Loss = {loss.detach().numpy() :.4f}')\n",
    "    l = 2/sigma2 * f_y_nl.reshape(-1, 1)\n",
    "    equivocation = loss_fn_bce(l, b_t).detach().numpy()/np.log(2)\n",
    "    eq_nonlin.append(equivocation)\n",
    "    print(f'equivocation nonlinear {equivocation:.2f}')\n",
    "\n",
    "    \n",
    "    \n",
    "    # Optimal\n",
    "    eq_nl = NonLinearEq(10)\n",
    "    optimizer = optim.Adam(eq_nl.parameters(), lr=0.001)\n",
    "    y_t = torch.Tensor(y.reshape(-1,1))\n",
    "    b_t = torch.Tensor(b.reshape((-1,1)))\n",
    "    # Trainings loop\n",
    "    for j in range(1000):\n",
    "        x_hat = eq_nl(y_t)\n",
    "        loss = loss_fn_bce(2/sigma2 * x_hat, b_t)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Printout and visualization\n",
    "        #if j % 50 == 0:\n",
    "            #print(f'epoch {j}: Loss = {loss.detach().numpy() :.4f}')\n",
    "    equivocation = loss.detach().numpy()/np.log(2)\n",
    "    eq_opt.append(equivocation)\n",
    "    print(f'equivocation optimal {equivocation:.2f}')\n",
    "    \n",
    "\n"
   ],
   "id": "9dc6001d",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e1b3f291"
   },
   "source": [
    "plt.plot(SNRdBs, eq_lin, label='linear MSE')\n",
    "plt.plot(SNRdBs, eq_nonlin, label='nonlinear MSE')\n",
    "plt.plot(SNRdBs, eq_opt, label='optimal')\n",
    "plt.xlabel(\"SNR [dB]\")\n",
    "plt.ylabel(\"Equivocation [bits]\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ],
   "id": "e1b3f291",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "de935412"
   },
   "source": [],
   "id": "de935412",
   "execution_count": null,
   "outputs": []
  }
 ]
}
