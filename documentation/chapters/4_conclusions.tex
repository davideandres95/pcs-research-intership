%!TEX root = ../main.tex
\chapter{Conclusions}\label{chap:conclusions}

In the light of the above, both autoencoder proposals for joint probabilistic and geometric shaping show close-to-optimal performance over the \cgls{awgn} channel. The common key for success is the choice of a loss function which would maximize the \cgls{mi} as well as making sure that the gradient w.r.t $P_M$ was correctly computed. Yet, the potential of the autoencoder approach is for training over complex channels such as optical fiber. It is over these channels where we expect that different implementations will exhibit different performance. The reason for this suspicion resides in the methods for optimizing the \cgls{mi}. Both \citep{Aref} and \cite{Stark} introduce the entropy of the source distribution into the loss function. This in turn has the effect of adding a complementary path to the computational graph for computing the gradient w.r.t $P_M$ without backpropagating through the channel model. Fortunately, the \cgls{awgn} channel is not particularly disruptive and therefore the effect of this shortcut is minimized. However, if the channel was to introduce, for example, an interference in the form of a pilot tone, the symbols close to it should be transmitted less frequently than the non-overlapping. For this reason, we believe that \cite{Aref} would not perform as well due to the non-differentiable sampling mechanism. Nonetheless, the Gumbel-Softmax trick does introduce numerical instabilities and further complexity, so \cite{Aref} has also some strengths.\\
Finally, training over other channel models is an open research direction with some challenges but promising outcomes.