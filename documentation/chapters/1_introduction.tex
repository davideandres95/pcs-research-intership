%!TEX root = ../main.tex
\chapter{Introduction}\label{chap:introduction}

The constant demand for higher capacity digital links has motivated the development of communication schemes which approach closer and closer the analytical limit of the channel capacity. According to the definition of channel capacity, sending a single bit per time-frequency slot is inefficient. For this reason, higher-order modulations like \cGls{ask} or \cGls{qam} are used for better efficiency. Under these modulation schemes, the receiver handles more than two signal points per real dimension; the set of signal points is known as constellation. However, these schemes present a constant-width gap to the capacity limit. This is due to the usage of both uniform and discrete probability distributions for the occurrence of the constellation points. While it is not possible to move away from the discrete distributions because of the digital nature of the communication, specific non-uniform distributions can lead to further capacity improvements. 

Constellation shaping thus, is a technique which seeks to optimize the distribution of the transmit symbols. Furthermore, this optimization unfolds into the improvement of the constellation points' location, their occurrence probability or both simultaneously. The first is known as geometric shaping while the latter is known as probabilistic shaping. In both cases, the goal is to maximize the \cGls{mi} $I(X;Y)$ of the channel input X and output Y by optimizing the constellation. This optimization problem arises from the definition of channel capacity $C$:
\begin{align}
\label{eqn:capacity}
	C = \max_{p(X)} I(X;Y)
\end{align}

Currently, the optimal $p(x)$ has only be found for specific channels, such as the \cGls{awgn}, as knowledge of the channel distribution $p(y|x)$ is required. Still, solving \ref{eqn:capacity} can become mathematically intractable despite knowing $p(y|x)$.

Here is where the application of deep-learning helps to find constellations which maximize $I(X;Y)$, without analytical knowledge of the channel. As shown in \cite{O'Shea}, the complete communication systems can be interpreted as an autoencoder. This approach tackles the physical layer by optimizing the end-to-end performance instead of the performance of the individual components by carefully choosing the loss function. Geometric shaping under the autoencoder framework has been already performed in \cite{O'Shea}, \cite{Jones}. Furthermore, geometric shaping and probabilistic shaping have been jointly implemented in \cite{Stark}, \cite{Aoudia} and \cite{Aref}. The results show that the joint application of probabilistic and geometric shaping outperform the PS-QAM scheme from \cite{Boecherer} and approach the limit to within 0.1 dB in the AWGN channel. Yet the existence of multiple and apparently different architectures is intriguing. Consequently, in this work we study the differences in the implementations proposed by \cite{Stark} and \cite{Aref}.

The rest of this document is organized as follows. Chapter 2, Preliminaries, presents the fundamentals of classical Probabilistic constellation systems as well as of autoencoder systems. Chapter 3, presents an implementation of both architectures with Pytorch \cite{PyTorch} and analyses their main differences. Chapter 4 wraps-up this work.
