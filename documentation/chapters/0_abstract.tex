%!TEX root = ../main.tex
\addchap{Abstract}\label{chap:abstract} % basically chapter*, but keeps the chapter in the table of contents
Finding sets of optimal parameters for communication systems with complex or unknown channel models is a problem which can be approached thanks to \cgls{ml}. These parameters refer to the location and probability of occurrence of the constellation points. Together with autoencoders, pioneered in \citep{O'Shea}, which optimize jointly transmitter and receiver; and the appropriate loss function, which must maximize the channel's mutual information, this \cgls{ml}-based method has shown close-to-optimal results in \cite{Stark} and \cite{Aref} when performed over the \cgls{awgn} channel. Yet, one particular challenge of this approach is learning a categorical distribution using a stochastic neural network, as backpropagating through the samples requires dealing with non-differentiable layers. In other words, learning a discrete probability distribution for the symbols occurrence requires that the sampling mechanism is differentiable, so that the gradient of the loss w.r.t the probability distribution can be computed via backpropagation. While the method proposed by \citeauthor{Stark} leverages the Gumbel-Softmax gradient estimator \cite{JANG}, \citeauthor{Aref} suggest an ad-hoc correction of this gradient after backpropagation. In this work, we break-down and analyze the benefits and drawbacks of each approach to further understand their potential and applicability over more complex channels.
