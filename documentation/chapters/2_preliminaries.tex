%!TEX root = ../main.tex
\chapter{Preliminaries}\label{chap:preliminaries}
\section{Probabilistic Constellation Shaping}
In this section our goal is to present the capacity limitations of the commonly used \cgls{ask} and \cgls{qam} modulation schemes. These schemes are penalized for two reasons:
\begin{enumerate}
\item They use uniform probability densities.
\item The constellation points are equidistant.
\end{enumerate}
In the following we explain the nature of these penalties.
\subsection{Introduction}
We begin with an important result from Information theory. Under a second-moment constraint, also known as power constraint, the probability distribution which maximizes the differential entropy is the Gaussian distribution, denoted with $p_G$. We thus have
\begin{align}
\label{eqn:max_entropy_scalar}
	h(X) \leq \dfrac{1}{2} \log \left(2 \pi e \sigma^2 \right)
\end{align}
where $\sigma^2 = \mathbb{E}[X^2]$, and with equality if and only if $X$ is Gaussian-distributed. More generally in the multi-dimensional case we have  
\begin{align}
\label{eqn:max_entropy_n}
	h(\underline{X}) \leq \dfrac{1}{2} \log \left((2 \pi e)^n |\textbf{Q}_{\underline{X}}| \right)
\end{align}
where we have considered a random column vector $\underline{X}$ of dimension n, mean $\mathbb{E}[\underline{X}]= \underline{m}$ and covariance matrix
\begin{align}
	\textbf{Q}_{\underline{X}}  = \mathbb{E}[(\underline{X} - \underline{m})(\underline{X} - \underline{m})^\intercal]
\end{align}
and equality in (\ref{eqn:max_entropy_n}) if only if the elements of $\underline{X}$ are jointly Gaussian.\\

\begin{figure}[ht]
\centering
\begin{tikzpicture}
    \node(src){};
    \node[circle, draw, inner sep=0cm]at (4,0) (plus){$+$};
	
	\node at (4,1.5)(z){$Z\sim\mathcal{N}(0,N)$};

	\draw[-latex] (src) --  (plus) node[midway,above] {$X\sim\mathcal{N}(0, P)$};
	\draw[-latex] (plus) -- node[midway,above] {$Y$} (6,0);	
	
	\draw[-latex](z)--(plus);
\end{tikzpicture}
\end{figure}

Lets now consider an \cgls{awgn} channel with Gaussian input $X$, of zero mean and variance $P$; Gaussian noise $Z$, of zero mean and variance $N$; and output $Y$; i.e. $Y = X + Z$. %A visual representation of the \cgls{awgn} channel is provided in Figure \ref{}.

Furthermore, the capacity of the \cgls{awgn} is
\begin{align}
\label{eqn:awgn_cap}
	C(P) &= \max\limits_{P_X:\mathbb{E}[X^2] \leq P} \mathbb{I}(X;Y)\\
	& = \max\limits_{P_X:\mathbb{E}[X^2] \leq P}[h(Y) - h(Y \vert X)]\\
	& = \dfrac{1}{2} \log (2 \pi e (P+N)) - \dfrac{1}{2} \log (2 \pi e N)\\
	& = \dfrac{1}{2} \log \left(1 + \dfrac{P}{N} \right). 
\end{align}
We can express the mutual information
\begin{align}
\label{eqn:MI}
	\mathbb{I}(X;Y) = h(Y) - h(Y \vert X)
\end{align}
in two parts, the differential entropy of the output and the conditional differential entropy of the output given the input.
We expand the second term as
\begin{align}
	h(Y \vert X) &= h(Y - X \vert X) \\
	& = h(Z \vert X)\\
	& = h(Z) = \dfrac{1}{2} \log \left(2 \pi e \sigma^2 \right)
\end{align}
and observe that the term $h(Y \vert X)$ does not depend on how $X$ is distributed. In contrast, $h(Y)$ does depend on how $X$ is distributed by
\begin{align}
	p_Y(y) = \int_{-\infty}^{\infty} p_X(X)p_Z(y -x) \,dx = (p_X \star p_Z)(y).
\end{align}
To circumvent the fact that it is difficult to find a closed-form expression of $h(Y)$, we make use of the information divergence as
\begin{align}
\label{eqn:hy_ce}
	h(Y) \overset{\text{(a)}}{=}  h(Y_G) - \mathbb{D}(p_Y \Vert p_G)
\end{align}
where (a) arises from the fact that $\mathbb{X}(p_X \Vert p_G) = h(Y_G)$ if and only if $p_X$ has zero mean and variance P as $p_G$. (\ref{eqn:hy_ce}) is very useful as it allows us to express the differential entropy of the output in terms of the information divergence between $p_G$ and any other distribution by means of the cross entropy. 

Now we can rewrite \ref{eqn:MI} as
\begin{align}
	\mathbb{I}(X;Y) &= h(Y) - h(Y \vert X)\\
	& = h(Y) - h(Z)\\
	& = h(Y_G) - \mathbb{D}(p_Y \Vert p_G) - h(Z)\\
	& = [h(Y_G) - h(Z)] - \mathbb{D}(p_Y \Vert p_G)\\
	& = C(P/\sigma^2) - \mathbb{D}(p_Y \Vert p_G).
\label{eq:C_minus_D}
\end{align}
This last result indicates that the loss of MI when using a distribution $P_X$ different than $P_G$ is the informational divergence $\mathbb{D}(p_Y \Vert p_G)$. In other words, if the gaussian distribution is not used, the capacity penalty is characterized by $\mathbb{D}(p_Y \Vert p_G)$.\\

\subsection{Capacity Gap for Uniform Continuous Input}
\begin{figure}[ht!]
\centering
\begin{tikzpicture}
    \node(src){};
    \node[circle, draw, inner sep=0cm]at (4,0) (plus){$+$};
	
	\node at (4,1.5)(z){$Z\sim\mathcal{N}(0,N)$};

	\draw[-latex] (src) --  (plus) node[midway,above] {$X\sim\mathcal{U}\left[-A, A\right]$};
	\draw[-latex] (plus) -- node[midway,above] {$Y$} (6,0);	
	
	\draw[-latex](z)--(plus);
\end{tikzpicture}
\end{figure}
We would like now to understand how far a uniform distribution is from (\ref{eqn:max_entropy_scalar}). To do this, we will follow the approach presented in \cite{BoechererCM} to lower bound the MI. Start by defining $X_u$ as a uniformly distributed input on the interval $[-A, A]$ where A is carefully chosen so that $\mathbb{E}[{X_u}^2] = P$. The corresponding output is $Y_u$ and we proceed
\begin{align}
	\mathbb{I}(X_u;Y_u) &= C(\text{snr}) - \mathbb{D}(p_{Y_u} \Vert p_{Y_G})\\
	& \geq C(\text{snr}) - \mathbb{D}(p_{X_u} \Vert p_{X_G})\\
	& = C(\text{snr}) -[h(X_G) - h(X_u)]\\
	& = C(\text{snr}) - \dfrac{1}{2} \log_{2} \left(\dfrac{\pi e}{6}\right).
	\label{eqn:gap}
\end{align}
In Figure \ref{fig:capacity_gap} we display the derived lower bound and observe that the capacity loss, originated from the use of a uniform input density, is never more than $\dfrac{1}{2}\log_2\dfrac{\pi e}{6}$ independent of the \cgls{snr}.
\begin{figure}
	\includegraphics[width=\textwidth]{figs/capacity_gap.png}
	\caption{AWGN channel capacity gap. The orange line indicates the upper capacity bound for any uniformly distributed constellation.}
    \label{fig:capacity_gap}
\end{figure}
To show that the shaping gap is tight, it is necessary to proof an upper bound for $\mathbb{I}(X_u;Y_u)$ that approaches \ref{eqn:gap} with increasing \cgls{snr}. We refer the reader to \cite{BoechererCM}, Section 4.3, for this proof.
\subsection{Uniform Discrete Input Bound}
We now show the penalty received for the use of an equidistant M-\cgls{ask} constellation. We define the constellation points as
\begin{align}
	\mathcal{X} = \{\pm\Delta 1, \pm\Delta 3,\dots, \Delta(M-1)\},
\end{align}
where $\Delta$ is the scaling factor of the constellation, so that the channel input is $X$. If $X_M$ is uniformly distributed, the resulting power  is
\begin{align}
	P = \mathbb{E}\left[X_{M}^2\right] = \Delta^2\frac{M^2 - 1}{3}.
\end{align}
\begin{theorem}[Uniform Discrete Input Bound]
\label{th:uniform_discrete_input_bound}
The mutual information achieved by $X_M$ is lowered bounded by
\begin{align}
\mathbb{I}(X_M;Y_M) &\geq \dfrac{1}{2} \log_2 \left( \dfrac{M^2}{M^2 -1}\right) - \dfrac{1}{2} \log_2 \left[2\pi e\left( \dfrac{P}{M^2 -1}+ \dfrac{P}{1+P/\sigma^2}\right)\right]\\
& < C(\text{snr}) - \dfrac{1}{2} \log_{2} \left(\dfrac{\pi e}{6}\right) - \dfrac{1}{2} \log_2 \left[1 +\left(\dfrac{2^{C(\text{snr})}}{M}\right)^2\right]\\
& < C(\text{snr}) - \text{Penalty}(\text{Uniform dist.}) - \text{Penalty}(\text{Equidistant dist.})
\end{align}
where $\text{snr} = P/\sigma^2$.
\end{theorem}
We refer the reader to \cite{BoechererCM}, section 4.5, for the proof.
Theorem \ref{th:uniform_discrete_input_bound} shows our goal, namely that both the usage of a uniform distribution and an equidistant constellation penalizes the capacity. We can additionally compute the relation between the constellation size, $M$, and $C(\text{snr})$ so that the resulting \cgls{mi} is within a constant gap of capacity. To make this result even more attractive, we increase the constraint for the gap to match the order of the distribution loss (0.255 bits). We obtain
\begin{align}
	- \dfrac{1}{2} \log_2 \left[1 +\left(\dfrac{2^{C(\text{snr})}}{M}\right)^2\right] \leq - \dfrac{\log_2 e}{2} \left(\dfrac{2^{C(\text{snr})}}{M}\right)^2 = \dfrac{1}{4}\\
	\Leftrightarrow M = 2^{C(\text{snr}) + \tfrac{1}{2}+\tfrac{1}{2} \log_2 \log_2 e}
\end{align}
by using $\log_e(x)\leq (1-x)$. So if 
\begin{align}
	\log_2 M \approx C(\text{snr}) +0.77,
\end{align}
then the mutual information is within 0.5 bit of capacity.

% Write about Maxwel-Boltxman distributions...
\subsection{Capacity-achieving distributions}
We now address the question of finding the discrete probability distribution which maximizes the capacity. Such distribution should be free of the $ \dfrac{1}{2} \log_{2} \left(\dfrac{\pi e}{6}\right)$ penalty. We use again an \cgls{ask} constellation with M signal points ( in practice, M is a power of 2) given by 
\begin{align}
	\mathcal{X} = \{\pm 1, \pm 3,\dots, (M-1)\}.
\end{align}

Let $X$ be a random variable with distribution $P_X$ over $\mathcal{X}$. As before, we scale $X$ by a $\Delta > 0$ and the resulting input/output relation for an \cgls{awgn} channel becomes
\begin{align}
	Y = \Delta X + Z
\end{align}

In consequence, the \cgls{mi} of the channel input and output is 
\begin{align}
	\mathbb{I}(\Delta X; Y) &= \mathbb{I}(\Delta X; \Delta X + Z)\\
	&= \mathbb{I}(\Delta X; \Delta X + Z)
\end{align}

where the second equality follows because $(\Delta X)$ is a deterministic function of $X$ and vice-versa. Under an input average power constraint $P$, the scaling $\Delta$ and the distribution $P_X$ must be chosen to satisfy
\begin{align}
	\mathbb{E}[(\Delta X)^2] \leq P.
\end{align}

Formally, our optimization problem is the following

\begin{align}
	C(P/\sigma^2) &= \max\limits_{\Delta, P_X :\mathbb{E}[(\Delta X)^2] \leq P} \mathbb{I}(X;\Delta X+Z).
\end{align}

Maximizing the mutual information $\mathbb{I}(X;\Delta X+Z)$ both over the scaling of the constellation points and the input distribution requires a relatively high amount of power. Instead, as shown in \cite{BoechererCM}, section 5.3, we will use a suboptimal input distribution which follows from maximizing the input entropy.

We expand the mutual information as
\begin{align}
	\mathbb{I}(X,\Delta X+Z) &= \mathbb{H}(X) - \mathbb{H}(X| \Delta X+Z)
\end{align}
and fixing $\Delta$, we select the input distribution $P_{X_\Delta}$ that maximizes the input entropy under our power constraint, i.e., we choose
\begin{align}
	P_{X_\Delta} &= \argmax\limits_{P_X:\mathbb{E}[(\Delta X)^2] \leq P} \mathbb{H}(X).
\end{align}
Without the discrete constraint, the solution would be a Gaussian distribution. For this reason we explore sampled Gaussian distributions, also known as \cgls{mb} distributions. For each $\mathcal{X} = \{\pm1, \pm 3,\dots, (M-1)\}$, define
\begin{align}
	P_{X_v}(x_i) &= A_{\nu}e^{-\nu{x_i}^2},\text{  } A_{\nu} = \dfrac{1}{\sum\limits_{i=1}^{M}e^{-\nu{x_i}^2}} 
\end{align}

We now show that $P_{X_\Delta}$ is given by
\begin{align}
P_{X_\Delta}(x_i) = P_{X_\nu}(x_i) \text{ with } \nu : \mathbb{E}[(\Delta X_{\nu})^2] = P
\end{align}

\begin{proof}
Consider the finite set $\mathcal{X} = {x_1, x_2, \dots , x_n}$ . Let $f$ be a function that
assigns to each $x_i \in \mathcal{X}$ a positive cost $f(x_i) > 0$. Define the \cgls{mb} distribution
\begin{align}
	P_{X_v}(x_i) &= A_{\nu}e^{-\nu f(x_i)},\text{  } A_{\nu} = \dfrac{1}{\sum\limits_{i=1}^{M}e^{-\nu f(x_i)}} 
\end{align}	
Let $P_X$ be some distribution on $\mathcal{X}$ with $ \mathbb{E}[f(X)] = P $. Choose $\nu : \mathbb{E}[f(X_{\nu})] = P$
\begin{align}
	0 &\leq \mathbb{D}(P_X \Vert P_{X_{\nu}})\\
	&= \sum\limits_{x \in \text{Support}(P_{X_{\nu}})} P_X \log(\dfrac{P_X(x)}{P_{X_{\nu}}(x)}) \\
	&= -\mathbb{H}(X) - \sum\limits_{x \in \text{Support}(P_{X_{\nu}})} P_X(x) \log(P_{X_{\nu}}(x)) \\
	&\overset{\text{(*)}}{=} -\mathbb{H}(X) - \sum\limits_{x \in \text{Support}(P_{X_{\nu}})} P_{X_{\nu}}(x) \log(P_{X_{\nu}}(x)) \\
	&= -\mathbb{H}(X) + \mathbb{H}(X_{\nu}) \\
	\mathbb{H}(X) &\leq \mathbb{H}(X_{\nu})
\end{align}
where the (*) marked step follows since both distributions produce the same moments for $log(P_{X_{\nu}}(x))$.
\end{proof}


	
\section{Autoencoders}
In the following we consider \cglspl{ffnn}, an specific type of \cgls{nn}, \cgls{sgd}, and autoencoders.

\subsection{Feed-Forward Neural Networks}
\cGlspl{ffnn} are structures which map an input vector $\bold{v_0} = (v_{0,1} \dots v_{0,M})$ to an output vector $\bold{v_k} = (v_{K,1} \dots v_{K,N})$, i.e., $\bold{v_K}=f_{\text{NN}}(\bold{v_0})$. This transformation is accomplished by composing functions, which in turn are computed by layers. The number of layers, K, is commonly referred to as depth of the network. And each layer can be composed of an specific number of units (neurons), M, often referred to as width of the layer. A representation of this structure is

\begin{figure}[H]
	\centering
	\input{figs/ffnn}
\end{figure}


Note that the width of the input and output layers must match the dimensions of the input and output vectors.\\
Mathematically, the output of a unit is parametrized by a weigth, $\bold{w}_{k,m}$, and a bias, $b_{k,m}$, and can be expressed as
\begin{align}
	v_{k,m} = g_{\text{NL},k}(\vb{v}_{k-1}\vb{w}^\top_{k,m} + b_{k,m}), \qquad k = 1, \dots, K \qquad m = 1, \dots, M.
	\label{eqn:neuron}
\end{align}
Where in (\ref{eqn:neuron}), k indicates the layer index, m indicates the unit index, and $g_{\text{NL},k}(\cdot)$ is a nonlinear function, e.g., \cgls{relu}, applied between layers.

\begin{figure}[H]
\tikzstyle{unit}=[draw,shape=circle,minimum size=1.15cm]
\centering
\begin{tikzpicture}[shorten >=1pt,->]
	\node[unit](p) at (2,1){$\Sigma$};
	\node(dots) at (-0.25,1){\vdots};

	\draw (2,2) node[yshift=10]{$b_{k,n}$} -- (p);
	
	\draw (-0.75,1.75) node[xshift=-20]{$v_{k-1,1}$} --(p);
	\draw (-0.75,1.75) node[xshift=1.2cm]{$w_{k-1,1}$} --(p);
	
	\draw (-0.75,0) node[xshift=-20]{$v_{k-1,M}$} -- (p);
	\draw (-0.75,0) node[xshift=1.2cm]{$w_{k-1,M}$} -- (p);
	
	\node[unit, scale=0.6](f) at (3.5,1){$g_{\text{NL},k}$};
	
	\draw (p) -- (f);
	\draw (f) -- (4.5,1) node[xshift=10]{$v_{k,m}$};
\end{tikzpicture}
	
\end{figure}
Indeed, without the nonlinear function the \cgls{ffnn} lacks the expressive power required to approximate any function \cite{HORNIK1989359}.\\
Finally, using matrix notation we can express the output of each layer as
\begin{align}
	\vb{v_k} = g_{\text{NL},k}(\vb{W}_k\vb{v}_{k-1}+\vb{b}_k), \qquad k = 1, \dots, K.
\end{align}

These structures, together with a training algorithm, help us to find parameters $\bold{W_k}$ and $\bold{b_k}$ such that $f_{\text{NN}}(\bold{v_0})$ approximates an unknown function. To this end, the algorithm receives training sets of observed pairs of the unknown function, where each pair has the form $\{$input, output$\}$.

\subsection{Stochastic Gradient Descent}
To find fitting sets of parameters $\{\bold{W_k}, \bold{b_k}\}$ we define a loss function,  $L(\bold{W_k} , \bold{b_k})$, that compares the current output of the \cgls{nn} with the desired output from the training set. The most used algorithm is \acrfull{sgd} which starts with a random set of initial values and then updates $\{\bold{W_k}, \bold{b_k}\}$ with each iteration as
\begin{align}
	\boldsymbol{\theta}_{new} = \boldsymbol{\theta}_{old} + \epsilon \grad L(\boldsymbol{\theta}_{old}), \qquad \boldsymbol{\theta} \in \{\bold{W_k}, \bold{b_k}\}
\end{align}

To compute the gradient effiently a computational graph stores the transformations to the factors which influenciate the loss function. 

\subsection{Autoencoders}
Autoencoders have been very successfull in fields like information compression, dimensionality reduction or computer vision. In the context of \cgls{ml} and communication systems it was pioneered by \citet{O'Shea}. The idea behind an autoencoder is to transmit a particular representation of the input data so that at the output, it can be reconstructed with minimal error. This means that the desired representations must be robust with respect to the channel impairments (i.e. noise, fading, distortion, etc.), referred to as the \textit{bottleneck} in the autoencoder jargon. To find such representations and the corresponding mappings, $\textbf{x}$ to $\textbf{y}$, we make use of two \cglspl{ffnn}: the encoder, performing $f(\textbf{x})$, and the decoder, performing $g(\textbf{y})$. %Figure \ref{fig:autoencoder} ilustrates these concepts.

\begin{figure}[h]
\centering
\input{figs/autoencoder}
\caption{Representation of the \cglspl{nn} of an autoencoder.}
\label{fig:autoencoder}
\end{figure}

One particular requirement for using \cgls{sgd} in autoencoders is that the loss gradient needs to be backpropagated all the way through receiver and channel to the transmitter. Otherwise, the transmitter parameters cannot be updated. This in turn means, that channel and receiver must be available as differentiable functions during training.
\section{Conclusion}
We can now set up the problem which we would like to address in this work. Namely, to train a \cgls{nn}-based autoencoder system to find optimal transmission and reception parameters. By maximizing the \cgls{mi} during training, the output distribution must satisfy (\ref{eq:C_minus_D}), and thus, approach the channel capacity without the mentioned penalties. Because \cglspl{nn} are universal function approximators \cite{HORNIK1989359}, this technique is paramount for finding the appropriate parameters over channels with a very complex, or even mathematically intractable, model.