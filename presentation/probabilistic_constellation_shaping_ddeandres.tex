\documentclass[english,aspectratio=1610,9pt,helvet,nicetitles]{ICEbeamerTUMCD}
% options: 169, 1610, 43, mathTUMCD, english, german, ngerman, helvet, handout, notes, ruled, nicetitles
% Unknown options are passed to beamer class, e.g., pass t for top alignment of slide content

\newcommand{\PersonTitel}{}
\newcommand{\PersonVorname}{David}
\newcommand{\PersonNachname}{de Andrés Hernández}
\newcommand{\PersonStadt}{Munich}
\newcommand{\PersonAdresse}{%
    Boschetsriederstr. 55A\\%
    81379~\PersonStadt%
}
\newcommand{\PersonTelefon}{@Telefon@}
\newcommand{\PersonEmail}{@E-Mail@}
\newcommand{\PersonWebseite}{@Web@}
% Fakultät:
\newcommand{\FakultaetName}{Department of Electrical and Computer Engineering}
\newcommand{\LehrstuhlName}{Institute for Communications Engineering}

\hyphenation{} % eigene Silbentrennung
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\Datum}{\today}


\title{Geometric and Probabilistic Constellation\\ \vspace{0.5cm} Shaping with Autoencoders}
\subtitle{Research Internship} % Comment out if no subtitle wanted
\author{\PersonVorname{} \PersonNachname{}}
\institute[]{Technical University of Munich \\ Institute for Communications Engineering}

% \setlength{\offsetTitle}{1cm} % Adjust spacing between title and header
% \setlength{\authorOffsetTitlepage}{5cm} % Adjust spacing between author and title

% More layouts of notes can be activated by uncommenting the following
% See https://github.com/gdiepen/latexbeamer-handoutWithNotes for all available layouts
% \pgfpagesuselayout{3 on 1 with notes}[a4paper,border shrink=5mm]
\graphicspath {{/home/ddeandres/Projects/internship_pcs/documentation/figs/}}
\usepackage[backend=bibtex,style=ieee-alphabetic,natbib=true, maxcitenames=2]{biblatex} %added
\usepackage{subfigure} %added
\usepackage{tikzscale} %added
\usepackage{xcolor}	%added
\usepackage{physics}%added
\setbeameroption{show notes on second screen}
\setbeamertemplate{note page}{%
  \vspace{10mm}
  \nointerlineskip
  \insertnote
}
\setbeamerfont{note page}{size=\normalsize}
\addbibresource{/home/ddeandres/Projects/internship_pcs/documentation/refs.bib}

\begin{document}
\setlength{\baselineskip}{\PraesentationAbstandAbsatz}
\setlength{\parskip}{\baselineskip}
% \let\thefootnote\relax\footnote
\PraesentationMasterStandard

\PraesentationTitelseite % Fügt die Startseite eon

\begin{frame}{Agenda}
  \begin{enumerate}
  \item Introduction
  	\begin{itemize}
  	\item SNR Gap
  	\item Probabilistic Constellation Shaping
  	\item Geometric Shaping
  	\end{itemize}
  \item Autoencoders
	\begin{itemize}
	\item Challenges
  	\end{itemize}
  \item Contribution
  \begin{itemize}
  	\item First Implementation
  	\item Second Implementation
  \end{itemize}
  \item Conclusions
  \end{enumerate}
  
\end{frame}


\begin{frame}{Introduction}
  \begin{itemize}
  \item We want to make use of each channel's capacity as efficiently as possible
  $$ C = \max_{p(X): \mathbb{E}[X^2] \leq P} \mathbb{I}(X;Y)$$
  \item The optimal $p(x)$ has only been found for specific channels, such as the AWGN, since knowledge of the channel distribution $p(y|x)$ is required.
  \end{itemize}
  \begin{minipage}{0.5\linewidth}
    \input{ressources/QAM_MI.tex}
  \end{minipage}%
  \begin{minipage}{0.5\linewidth}
    \input{ressources/SNR_GAP.tex}
  \end{minipage}
  \note{
  \begin{itemize}
  \item Simple problem definition: maximize the MI under a power constraint 
  \item Sending a single bit is inefficient. For this reason, QAM and ASK schemes are used. So depending on the SNR you are targetting you might choose the cardinality of your scheme.
  \item However, if we are to improve the AIR of these schemes we will soon find out that there are some barriers.
  \end{itemize}
  }
  
\end{frame}

\begin{frame}{Closing the SNR Gap}
	\begin{itemize}
	\item ASK and QAM modulation schemes are penalized for two reasons:
	\begin{enumerate}
		\item They use uniform probability densities
		\item The constellation points are equidistant
	\end{enumerate}
	\item Solution 1: Shape the probability of occurrence of the constellation points --- Probabilistic Constellation Shaping
	\item Solution 2: Shape the spatial location of the constellation points --- Geometric Constellation Shaping
	\begin{figure}
	\includegraphics[width=0.4\columnwidth]{ressources/PCS_GCS.png}
	\caption{(a) Probabilistic Constellation Shaping, (b) Geometric Constellation Shaping; \cite{Cho2019ProbabilisticCS}}
	\end{figure}
	\end{itemize}

\end{frame}


\begin{frame}{Autoencoders}
\begin{itemize}
\item \citet{O'Shea} pioneered the idea of interpreting the complete communication system as an autoencoder
\item Idea: transmit a particular representation of the input data so that at the output, it can be reconstructed with minimal error
\item This representations must be robust with respect to the channel impairments (i.e. noise, fading, distortion, etc.) --- bottleneck in the autoencoder jargon

\end{itemize}
\vspace{-5mm}
\begin{figure}
\centering
\resizebox{!}{11em}{
\input{ressources/autoencoder}
}
\caption{Representation of the NN of an autoencoder.}
\label{fig:autoencoder}
\end{figure}
\note{
Why does it makes sense to use ML for this problem?
\begin{itemize}
\item Finding the constellation parameters when $p(y|x)$ is very complex or unknown can be mathematically untractable
\item NN have the property of being universal function approximators \cite{HORNIK1989359}
\item The autoencoder is implemented using Feed-Forward Neural Networks (FFNN), and the parameters are learned using Stochastic Gradient Descent (SGD)
\item --- ---
\item An autoencoder is a type of neural network used to learn efficient codings of unlabeled data (unsupervised learning).
\item The autoencoder learns a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore insignificant data (“noise”).
\item The chosen encoding is validated and refined by attempting to regenerate the input from the encoding.
\end{itemize}
}
\end{frame}

\begin{frame}{Challenges}
	\begin{figure}
    \centering
	\begin{tikzpicture}
		\node[draw] at (0,1.5) (dist){Categorical Distribution};
		\node[draw, style=dashed, color=red] at (0,0) (sampler){Sampler};
		\node[draw, label=below:$f_{\textcolor{blue}{C_M}}(s)$]  at (3,0) (map){Mapper};
		\node[draw, label=below:$P_{Y|X}(\cdot|x)$] at (6,0) (ch) {Channel Model};
		\node[draw, label=below:$Q_{X|Y}(\cdot|y) \equiv \textcolor{blue}{D}$] at (9,0) (demap) {Demapper};
		\node(dst) at (11,0){};
		
		\draw[-latex] (dist) --  (sampler) node[midway,right] {$\textcolor{blue}{P_M}$};	
		\draw[-latex] (sampler) --  (map) node[midway,above] {$s$};		
		\draw[-latex] (map) --  (ch) node[midway,above] {$x$};
		\draw[-latex] (ch) -- (demap) node[midway,above] {$y$};	
		\draw[-latex] (demap) --(dst) node[right, above, color=red]{Loss};

	\end{tikzpicture}
	\end{figure}
	\vspace{-5mm}
	\begin{itemize}
	\item To find fitting sets of parameters $\{\textcolor{blue}{P_M, C_M, D}\}$ we define a loss function,  $L(P_M, C_M, D)$, 	and update the parameters using SGD.
	\item Geometric shaping using autoencoders is a solved problem \citep{O'Shea}, \citep{Jones}.
	\item Probabilistic shaping is different because of the added stochastic node (sampler).
	\end{itemize}
	\note{
	\begin{itemize}
	\item To find fitting sets of parameters $\{\textcolor{blue}{P_M, C_M, D}\}$ we define a loss function,  $L(P_M, C_M, D)$, that compares the current output of the autoencoder with the desired output from the training set.
	\item To compute the gradient efficiently a computational graph stores the transformations to the factors which influenced the loss function
	\item This requires \textbf{every block} to be \textbf{differentiable}
	\item Automatic differentiation (pytorch cannot handle complex numbers)
	\end{itemize}
	}
\end{frame}



\begin{frame}{First implementation \cite{Stark}}
	Trainable parameters:
	\vspace{-5mm}
	\begin{itemize}
		\item $P_M$, source's probability distribution learnt by the encoder.
		\item $C_M$, spatial distribution of the constellation points learnt by the mapper.
		\item $D$, posterior probability distribution learnt by the demapper.
	\end{itemize}
	\vspace{-5mm}
	Autoencoder architecture:
	\begin{figure}
		\centering
		\includegraphics[width=0.6\columnwidth]{stark_diagram.pdf}
%		\caption{Proposed Autoencoder Architecture from \cite{Stark}}
		\label{fig:starkAe}
	\end{figure}
\end{frame}

\begin{frame}{Gumbel-Softmax trick \cite{JANG}}
\begin{itemize}
\item Solves the problem of backpropagating through stochastic nodes by reparametrizing the samples to avoid breaking the dependency between the samples and the trainable parameters.
\item Relaxes the argmax function using a softmax instead, which is smooth in $\tau > 0$. The parameter $\tau$, controls the degree of approximation to the expected value of the categorical distribution.
\end{itemize}
\begin{figure}
\includegraphics[width=0.85\columnwidth]{ressources/Gumbel_Softmax.pdf}
\caption{ Non-differentiable path vs. differentiable path using the Gumbel-Softmax trick. }
\end{figure}
\end{frame}

\begin{frame}{Loss Function}
	The goal of probabilistic constellation shaping is to maximize the MI. To this end, defining an appropriate loss function is critical. Starting from the demodulator, the categorical cross entropy loss
\begin{align*}
	L(D, P_M, C_M) \triangleq \mathbb{X}(P_{X|Y}||Q_{X|Y}; D) = \mathbb{E}\left[-\log_2(Q(X|Y;D))\right] 
\end{align*}
is appropriate for training $D$ and $C_M$, but not $P_M$.\pause To see why, we rewrite the MI as
\begin{align*}
	\mathbb{I} \left(X , Y\right) &= \mathbb{H}(X) - \mathbb{H}(X|Y)\\
	\mathbb{I} \left(X , Y\right) &= \mathbb{H}(X) - \mathbb{X}(P_{X|Y}||Q_{X|Y}) + \mathbb{D}(P_{X|Y}||Q_{X|Y}).
\end{align*}\pause
And the Loss becomes
\begin{align*}
\textcolor{red}{L(D, P_M, C_M)} &= \underbrace{\mathbb{H}(X)}_{!} -\mathbb{I} \left(X , Y\right) + \mathbb{D}(P_{X|Y}||Q_{X|Y}).
\end{align*}
So, if $L$ is minimized during training, the source entropy is unwantedly minimized.
%If we minimize the cross entropy loss we would also minimize the source entropy. A modification of this loss function is necessary.
\end{frame}

\begin{frame}{Loss Function (cont'd)}
To avoid this effect, \citeauthor{Stark} modify the loss function as
\begin{align*}
	\textcolor{blue}{\hat{L}(D, P_M, C_M)} \triangleq \textcolor{red}{L(D, P_M, C_M)} - \mathbb{H}(X).
\end{align*}
With this correction the optimization problem 
\begin{align*}
	\min_{D, P_M, C_M}\textcolor{blue}{\hat{L}(D, P_M, C_M)} = \max_{D, P_M, C_M} \{ \mathbb{I} \left(X , Y\right) - \mathbb{D}(P_{X|Y}||Q_{X|Y})\}
\end{align*}
maximizes the MI.
\note{
\begin{itemize}
\item Training the E2E system by minimizing L corresponds to miximizing the MI, while minimizing the ID between the true posterior distribution and the one learned by the receiver.
\end{itemize}
}
\end{frame}


\begin{frame}{Learnt Constellations}
\begin{columns}[T,onlytextwidth]
		\begin{column}{.5\textwidth}
        \begin{figure}[h]
		    \input{ressources/stark_pcs_5db}
		    \label{subfig:stark5dB}
		    \\
	        \input{ressources/stark_pcs_18db}
	        \label{subfig:stark18dB}
		    %\caption{Learnt joint geometric and probabilistic ASK constellations for M=8.}
		    \label{fig:starkMASK}
		\end{figure}
        \end{column}
        \begin{column}{.5\textwidth}
        \begin{figure}[h]
		    \input{ressources/stark_gcs_5db}
		    \label{subfig:stark5dB}
		    \\
	        \input{ressources/stark_gcs_18db}
	        \label{subfig:stark18dB}
		    %\caption{Learnt joint geometric and probabilistic ASK constellations for M=8.}
		    \label{fig:starkMASK}
		\end{figure}
        \end{column}
\end{columns}
\end{frame}

\begin{frame}{Overall Performance}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\columnwidth]{stark_gcs.pdf}
%    \caption{Mutual information learned by the PCS and GeoPCS for constellation size M=64 on the AWGN channel. Zoom is for the 18dB point.}
    \label{fig:starkPerf}
\end{figure}
\end{frame}

\begin{frame}{Second implementation \cite{Aref}}
	Motivation:	\textit{The Gumbel-Softmax trick is complex and numerically unstable.\\}
	Claim: The sampler does not need to be differentiable as long as we have other means of computing the gradient w.r.t $P_M$\\
	Trainable parameters:
	\vspace{-5mm}
	\begin{itemize}
		\item $P_M$, source's probability distribution learnt by the sampler.
		\item $C_M$, spatial distribution of the constellation points learnt by the mapper.
		\item $D$, posterior probability distribution learnt by the demapper.
	\end{itemize}
	
    \begin{figure}
		\centering
		\includegraphics[width=0.7\columnwidth]{aref_diagram.png}
	%	\caption{Proposed Autoencoder Architecture from \cite{Aref}}
		\label{fig:arefAe}
	\end{figure}
\end{frame}

\begin{frame}{Loss Function}
	The goal is to to maximize the achievable transmission rate
	\begin{align}
		 \max_{D, P_M, C_M} \mathbb{R}_{PS} \left(X , Y ; D, P_M, C_M \right) = \max_{D, P_M, C_M} \left\lbrace \mathbb{H}(X) - \mathbb{X}(P_{X|Y} \Vert Q_{X|Y} ; D, P_M, C_M)\right\rbrace.
	\end{align}
	And the rate can be numerically approximated by
	\begin{align}
		\mathbb{R}_{PS} \left(X , Y\right) \approx \mathbb{R}_{PS} \left(X , Y\right)_{\text{num}} &= \dfrac{1}{B} \sum \limits_{i = 1}^{B} \underbrace{- \log_2(P(x_i)) + \log_2(Q_{X|Y}(x_i|y_i))}_{L(x_i, y_i)}
	\end{align}
	Next, the following approximation usually allows to adjust the trainable parameters:
	\begin{align}
		\pdv{\theta} \mathbb{R}_{PS} \left(X , Y ; \theta \right) \approx \pdv{\theta} \mathbb{R}_{PS} \left(X , Y\right)_{\text{num}} = \dfrac{1}{B} \sum \limits_{i = 1}^{B} \pdv{\theta} L(x_i, y_i).
	\end{align}
\end{frame}

\begin{frame}{Loss Function (Cont'd)}
	However, although this is true for the constellation locations $(\theta \in C_M)$ and the demapper parameters $(\theta \in D)$, it does not hold for the constellation probabilities $\{p_1, p_2, \dots, p_M\} = P_M$
	\begin{align}
	\label{eqn:mi_pdv_p}
		\pdv{p_j} \mathbb{R}_{PS} \left(X , Y ; P_M \right) \not\approx \dfrac{1}{B} \sum \limits_{i = 1}^{B} \pdv{p_j} L(x_i, y_i)
	\end{align}
	
	as $\{p_1, p_2, \dots, p_M\}$ changes the statistics of the training set.
	
	\end{frame}

\begin{frame}{Loss Function (Cont'd)}	
\begin{itemize}
 	\item The derivative of the rate w.r.t. $p_j$, (\ref{eqn:mi_pdv_p}), results to be
	\begin{align}
		\pdv{p_j} \mathbb{R}_{PS} \left(X , Y ; P_M \right) \approx \underbrace{- \log_2 (p_j)}_\text{backpropagation} - \log_2 (e) + \underbrace{\dfrac{1}{Bp_j}\sum \limits_{b \text{ if } x=j} \log_2 Q_{X|Y}(j|b)}_\text{backpropagation} + \dfrac{1}{B} \sum \limits_{(a,b)} \log_2 Q_{X|Y}(a|b)
	\end{align}
	
	\item The following terms can be computed via backpropagation
	\begin{align}
		- \log_2 (p_j) + \dfrac{1}{Bp_j}\sum \limits_{b \text{ if } x=j} \log_2 Q_{X|Y}(j|b) = \dfrac{1}{B} \sum \limits_{i = 1}^{B} \pdv{p_j} L(x_i, y_i)
	\end{align}
	while the remaining ones must be explicitly computed and added to the gradient after backpropagating.
	\item We call this step \textit{gradient correction} and it is due to the change of statistics in the sampled batch.
\end{itemize}
\end{frame}
	

\begin{frame}{Learnt Constellations}
\begin{columns}[T,onlytextwidth]
		\begin{column}{.5\textwidth}
        \begin{figure}[h]
		    \input{ressources/aref_pcs_5db}
		    \label{subfig:arefk5dB}
		    \\
	        \input{ressources/aref_pcs_18db}
	        \label{subfig:aref18dB}
		    %\caption{Learnt joint geometric and probabilistic ASK constellations for M=8.}
		    \label{fig:arefQAM}
		\end{figure}
        \end{column}
        \begin{column}{.5\textwidth}
        \begin{figure}[h]
		    \input{ressources/aref_gcs_5db}
		    \label{subfig:aref5dB}
		    \\
	        \input{ressources/aref_gcs_18db}
	        \label{subfig:aref18dB}
		    %\caption{Learnt joint geometric and probabilistic ASK constellations for M=8.}
		    \label{fig:arefMASK}
		\end{figure}
        \end{column}
\end{columns}
\end{frame}

\begin{frame}{Overall Performance}
	\centering
	\includegraphics[width=0.75\columnwidth]{aref_gcs.pdf}
	\label{fig:arefPerf}

\end{frame}

\begin{frame}{Conclusions}
\begin{itemize}
\item Both autoencoder proposals show close-to-optimal performance over the AWGN channel. \pause
\item Common keys for success:
	\begin{enumerate}
	\item the choice of the loss function
	\item and correct computation of the gradient w.r.t $P_M$
\end{enumerate}	\pause
\item Main differences:
	\begin{enumerate}
	\item \citeauthor{Stark} use the Gumbel-Softmax Trick to allow backpropagation through the sampler
	\item \citeauthor{Aref} use a non-differentiable sampling mechanism but compute the gradient manually
\end{enumerate}	 \pause
\item The potential of the autoencoder approach is for training over complex channels such as optical fiber.\pause
\item Both implementations introduce $\mathbb{H}(X)$ into the loss function. For the 2nd implementation this has the effect of adding a complementary path to the computational graph for altering the tensor's gradient w.r.t $P_M$ without backpropagating through the channel model.
\end{itemize}
\note{
Over other channels we expect that they will exhibit different performance.
}
\end{frame}

\begin{frame}{Bibliography}
\printbibliography[heading=none]
\end{frame}

\begin{frame}
\begin{Huge}
Back-up Slides
\end{Huge}
\end{frame}

\begin{frame}{Why does it makes sense to use ML for this problem?}
\begin{itemize}
\item Finding the constellation parameters when $p(y|x)$ is very complex or unknown can be mathematically untractable
\item NN have the property of being universal function approximators \cite{HORNIK1989359}
\item \citet{O'Shea} pioneered the idea of interpreting the complete communication system as an autoencoder
% e2e optimization of transmitter and receiver
\end{itemize}
\begin{figure}
\includegraphics[width=0.5\columnwidth]{ressources/oshea.png}
\caption{Autoencoder architecture proposed by \citep{O'Shea}.}
\end{figure}
\note{
\begin{itemize}
\item An autoencoder is a type of neural network used to learn efficient codings of unlabeled data (unsupervised learning).
\item The autoencoder learns a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore insignificant data (“noise”).
\item The chosen encoding is validated and refined by attempting to regenerate the input from the encoding.
\end{itemize}
}
\end{frame}

\begin{frame}{Autoencoders}
\begin{itemize}
%\item Autoencoders have been successfully implemented in fields like computer vision or data compression.
\item Idea: transmit a particular representation of the input data so that at the output, it can be reconstructed with minimal error
\item This representations must be robust with respect to the channel impairments (i.e. noise, fading, distortion, etc.) --- bottleneck in the autoencoder jargon
\item The autoencoder is implemented using Feed-Forward Neural Networks (FFNN), and the parameters are learned using Stochastic Gradient Descent (SGD)
\end{itemize}
\vspace{-5mm}
\begin{figure}
\centering
\resizebox{!}{11em}{
\input{ressources/autoencoder}
}
\caption{Representation of the NN of an autoencoder.}
\label{fig:autoencoder}
\end{figure}

\end{frame}

\begin{frame}{Probabilistic Constellation Shaping (1st Imp)}
\begin{figure}
	\subfigure[5dB]{
		\input{../documentation/figs/stark_pcs_5db}
		\label{subfig:stark_pcs_5db}
	}
	\subfigure[18db]{
		\input{../documentation/figs/stark_pcs_18db}
		\label{subfig:stark_pcs_18db}
	}
	\caption{Learnt probabilistic constellation shaping for M = 64. The size of the markers is proportional to the transmission probability of the symbol. When trained under 5dB, the probabilistic shaping approaches a Gaussian. While under 18dB it approaches a uniform distribution. }
\end{figure}
\end{frame}

\begin{frame}{Joint Probabilistic and Geometric Shaping (1st Imp)}
\begin{figure}[h]
	\subfigure[SNR = 5dB]{
         \input{../documentation/figs/stark_gcs_5db}
         \label{subfig:stark5dB}
    }
    \subfigure[SNR = 18dB]{
         \input{../documentation/figs/stark_gcs_18db}
         \label{subfig:stark18dB}
    }
    \caption{Learnt joint geometric and probabilistic ASK constellations for M=8.}
    \label{fig:starkMASK}
\end{figure}
\end{frame}



\begin{frame}{Probabilistic Shaping (2nd Imp)}
	\input{../documentation/figs/aref_QAM.tex}

\end{frame}

\begin{frame}{Joint Geometric and Probabilistic Shaping (2nd Imp)}
	\input{../documentation/figs/aref_MASK.tex}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document} % !!! NICHT ENTFERNEN !!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
